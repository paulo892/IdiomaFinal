\chapter{Conclusion and Future Work \label{ch:conclusion}}

\section{Conclusion}
In its current state, Idioma provides an excellent user experience that achieves all of the objectives that we deemed necessary to fill the void left by current available language-learning applications. On the one hand, its reliance on natural language processing and machine learning techniques satisfied our desire for a cutting-edge solution; though many comparable services leverage cutting-edge tech, Idioma's recent development window and direction allowed it to make use of burgeoning models and frameworks in its delivery of a state-of-the-art user experience. On the other hand, the application also delivers on the promise of a dynamic, customizable, reading comprehension-driven platform. The ability to tailor the reading content allows the user to take charge of their learning, while the proprietary algorithm constantly working on the back-end ensures that the service stays up-to-date with a user's strengths and weaknesses. All the while, students get the chance to experience these features through an expansive database of articles, varying in difficulty and in subject matter, that give them the practice in reading comprehension that they need to truly become proficient in the Portuguese language. Despite its limitations, there truly does not exist a service like Idioma on the market, and we are very proud to have delivered the foundation to this unique premise.

At the time of writing, all of the code related to Idioma and its constituent services is available for download in a publically-available GitHub repository. The link to this repository is as follows: https://github.com/paulo892/IdiomaFinal. It is our hope that future contributors will be able to help address the many opportunities for future work that are currently available, which this paper will now detail.

\section{Future Work}

While Idioma's suite of features is larger than even we had expected at the outset, there yet exist a wide variety of opportunities for future contributions and extensions to the project. 

Let us first consider those elements that are external to the application itself, yet critical to its proper functioning: the web scraper and the modeling suite.

The web scraper offers one exceptionally clear opportunity for future work, namely due to the fact that it is presently limited in scope and easily extensible. At the time of writing, the web scraper was tailored only to one journalistic website: \textit{Diário Notícias}. In theory, these scripts could be adapted to perform the same functions on any website, adding a greater variety of voices and subjects to the Idioma document database. Furthermore, the web scraper was last run in mid-March; therefore, the documents that a current user would see are actually fairly out-of-date, at least from a current events perspective. Therefore, any interested contributor could take on the task of extending the web scraper to pull from other sources and to do so more frequently, to ensure a more current pool of documents. They might even add a new functionality to allow the user to request specific websites, or even pull the data themselves on the fly; this would confer upon the user an even finer level of control over their learning. In practice, however, this task is not as easy as it appears. While very user-friendly, Scrapy is not a trivial library to develop upon; it is very dependent on the individual intricacies of a webpage's HTML structure, and so the inclusion of other sources in its execution path would require a fair amount of development time. This is compounded by the tendency of these news sites to use moderately complex JavaScript on their pages, which necessitates the use of Selenium requests on the part of the developer to gather the necessary data. These details make the scraping process more challenging than expected, and make us more confident in the success of our existing application.

The modeling suite is another area ripe for further contribution. Though not a limitation, as we believe the best-of-three approach that arose from the modeling suite performs quite well, it would be straightforward for an individual to conduct their own research and implement new additions as they see fit. Machine learning is a burgeoning field, and the current literature is constantly being extended to include new permutations of older models as well as completely novel approaches. This component in the Idioma platform is limited insofar as it sources only from those techniques that are viable today, as well as those implementations available for adaptation in the scikit-learn library. In addition to adapting new breakthroughs in binary classification, an individual might simply take the time to incorporate models for which scikit-learn does not yet offer a suitable implementation. Though this would take a fair amount of time, especially if these models have to be constructed from scratch, it could be a worthwhile opportunity for future extension.

Similarly, the data used to train these models could also be further refined. We are incredibly grateful to Filho for offering us his data set; without his assistance, this project would be nowhere near complete today, and it actually works very well in using only the data that he provided. Nevertheless, the target labels that this data set offers, namely 'd' for "difficult" and 's' for "simple," are not as useful as we had hoped. It is challenging to determine whether a user is ready for difficult documents, as this classification does not follow a universal standard that can be easily calculated for a given student. A future contributor could, therefore, take it upon themselves to craft a data set that labels documents according to CEFR guidelines, which would enable the developer to simply compute a user's proficiency level according to those metrics and offer a more accurate learning experience. Another limitation pertaining to the data is the absence of featurization methods for syntactical and POS attributes in a candidate text. We were not able to develop a module to efficiently break down a sentence into its constituent parts, a task that would have enabled us to utilize the majority of features in the data set provided by Filho. The addition of such a module could therefore have a tremendous effect on the accuracy of the modeling suite.

Let us now move on to the application itself. This version of Idioma has a fairly extensive feature set, with each of the constituent functionalities working as expected. That said, there are a few opportunities for future work ready to be tackled. The first is the process of hosting the application. Initially, it was our goal to launch the Idioma alpha on a publically-accessible domain. Due to time constraints, however, this became infeasible. Given how user-friendly services like Heroku have become, however, it should be straightforward for one to migrate the front- and back-end onto this service. 

The authentication system offers another occasion for extension. Though it can successfully authenticate a user whose credentials have been supplied to the Atlas DB, the portal is not configured to use Google Sign-in, nor can it provision accounts for interested new users. In addition, the current solution for storing user credentials (i.e. plain-text in an Atlas cluster) is frankly insecure; it would require at least some kind of hashing and salting algorithm to provide the user with security, which is not currently the case. These additions would be welcome additions to the platform.

Additionally, the User object within the application proper could be fleshed out as well. For one, there is currently no system in place for constructing a profile for a new user. We considered the idea of introducing some sort of quiz for a new student to take, which would provide some kind of feedback to inform the construction of their profile. Due to time constraints, however, we were unable to develop such a process, and the users presently included in the Atlas cluster were configured essentially at random. The User object is also limited by the lack of an algorithm to determine the appropriate skill level (i.e. 's' or 'd'). As noted above, the ambiguity of these categories makes the division of users between them a fairly difficult problem. As such, a future contributor could pursue a method to reliably do just that, or attempt to revise the training data such that the classification rests on more easily-differentiable target labels.

One final opportunity is more algorithmic in nature. Once again due to time constraints, we were unable to develop the feature allowing users to request documents based on their subject matter. In theory, this would be a straightforward addition. Along with the article text and metadata, our web scraper retrieves a list of tags describing the material as well. A handful of these tags actually correspond directly to a set of larger categories listed on the \textit{Diário Notícias} homepage, while the others are more free-form in nature. Nevertheless, one could simply collate these descriptors into a dictionary of categories, from which a user could select the most interesting for their learning.

That being said, we are incredibly happy with the foundation that has been laid through this project. Though incomplete, Idioma achieves the objectives that it set out to accomplish, providing a high-tech, flexible solution to the problem of reading comprehension in Portuguese second-language learning. We are confident that, with a bit of extra time and care, Idioma could become a true asset for industrious students around the world (especially those who are socially distancing).
